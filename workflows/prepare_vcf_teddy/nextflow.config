params {
  aggregate = "ComputeAlleleCountsAndHistograms"
  qc_metrics = "ABE ABZ BQZ CYZ FIBC_I FIBC_P HWE_SLP_I HWE_SLP_P IOR NM0 NM1 NMZ STZ SVM"

  // Use NO_FILE to indicate the optional samples list will be generated from samples in vcf.
  samples_path    = "NO_FILE"

  // Path glob to get both the bcf and the index files as pairs
  vcfs_glob       = "data/vcfs_doped/*.{bcf,bcf.csi}"

  vep {
    exec = "vep"
    flags = "--sift b --polyphen b --ccds --uniprot --hgvs --symbol --numbers" +
    " --domains --regulatory --canonical --protein --biotype --af --af_1kg" +
    " --af_esp --af_gnomad --pubmed --shift_hgvs 0 --allele_number --buffer_size 50000"

    loftee_path              = "/opt/loftee"
    loftee_human_ancestor_fa = "/opt/loftee/data/human_ancestor.fa.gz"
    loftee_conservation_file = "/opt/loftee/data/phylocsf_gerp.sql"
    loftee_gerp_bigwig       = "/opt/loftee/data/gerp_conservation_scores.homo_sapiens.GRCh38.bw"
  }

  cadd {
    script = "bin/add_cadd_scores.py"
    tsv_path = "data/cadd_annotations/chr11.trim.cadd.tsv.gz"
  }

  percentiles {
     exec = "ComputePercentiles"
     qc_metrics = ["ABE", "ABZ", "BQZ", "CYZ", "FIBC_I", "FIBC_P", "HWE_SLP_I",
       "HWE_SLP_P", "IOR", "NM0", "NM1", "NMZ", "STZ", "SVM", "QUAL"]
  }
}

process { }

executor {
  $slurm {
    queueSize = 1000
    jobName = { "bravo_vcf" }
    queue = "topmed"
    cpus = 1
    memory = "8 GB"
    time = "14d"
  }
  $local {
    // Number of cores to use for processes.
    cpus = 3 
  }
}

// To run on cluster use: nextflow run PrepareVCF.nf -profile slurm
profiles {
  standard {
    process.executor = 'local'
  }

  slurm {
    process.excutor = 'slurm'
  }
}
