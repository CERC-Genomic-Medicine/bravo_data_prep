params {
  aggregate = "ComputeAlleleCountsAndHistograms"
  qc_metrics = "ABE ABZ BQZ CYZ FIBC_I FIBC_P HWE_SLP_I HWE_SLP_P IOR NM0 NM1 NMZ STZ SVM"

  // Use NO_FILE to indicate the optional samples list will be generated from samples in vcf.
  samples_path    = "NO_FILE"

  // Path glob to get both the bcf and the index files as pairs
  bcfs_glob       = "data/bcfs/*.{bcf,bcf.csi}"

  vep {
    flags = "--sift b --polyphen b --ccds --uniprot --hgvs --symbol --numbers" +
    " --domains --regulatory --canonical --protein --biotype --af --af_1kg" +
    " --af_esp --af_gnomad --pubmed --shift_hgvs 0 --allele_number --buffer_size 50000"

    loftee_path              = "/apps/loftee"
    loftee_human_ancestor_fa = "loftee_data/human_ancestor.fa.gzi"
    loftee_conservation_file = "loftee_data/loftee.sql"
    loftee_gerp_bigwig       = "loftee_data/gerp_conservation_scores.homo_sapiens.GRCh38.bw"
    ref_fasta                = "/apps/reference/hs38DH.fa"
    cache                    = "/apps/vep_cache"
  }

  cadd {
    script = "/apps/data_prep/tools/py_tools/add_cadd_scores.py"
    tsv_path = "cadd/whole_genome_SNVs.tsv.gz"
  }

  percentiles {
     qc_metrics = ["ABE", "ABZ", "BQZ", "CYZ", "FIBC_I", "FIBC_P", "HWE_SLP_I",
       "HWE_SLP_P", "IOR", "NM0", "NM1", "NMZ", "STZ", "SVM", "QUAL"]
  }
}

executor {
  $slurm {
    queueSize = 1000
    cpus = 1
    memory = "6GB"
  }
  $local {
    // Number of cores to use for processes.
    cpus = 3 
  }
}

// To run on cluster use: nextflow run PrepareVCF.nf -profile slurm
profiles {
  standard {
    process.executor = 'local'
  }

  slurm {
    process.executor = 'slurm'
    process.queue = "bravo"
    process.time = "14d"
    process.module = ['vep', 'htslib','samtools', 'bam_util', 'data_prep', 'python3', 'bcftools']
  }
}
