params {
  // Use something like chr11.*.bcf for list of bcf files.
  chromosome      = "chr11"

  // Required input data
  vcfs            = "data/chr11.subset.bcf"
  // Use *.tsv.gz for list of cadd files.
  cadd_tsvs       = "data/chr11.sites.cadd.tsv.gz" 

  // Assume symlinks in workflow directory:
  //   loftee pointing to loftee directory e.g. loftee -> /opt/loftee
  //   loftee_data pointing do backing data directory e.g. loftee_data -> /opt/loftee_data

  loftee_flags    = "--sift b --polyphen b --ccds --uniprot --hgvs --symbol --numbers --domains --regulatory --canonical --protein --biotype --af --af_1kg --af_esp --af_gnomad --pubmed --shift_hgvs 0 --allele_number --buffer_size 10000"
  loftee_path     = "loftee"
  loftee_human_ancestor_fa = "loftee_data/human_ancestor.fa.gz"
  loftee_conservation_file = "loftee_data/phylocsf_gerp.sql"
  loftee_gerp_bigwig       = "loftee_data/gerp_conservation_scores.homo_sapiens.GRCh38.bw"

  window_size_bp  = 100000

  // Optional input data
  // Use NO_FILE to indicate the optional samples list is not being used.
  samples         = "NO_FILE"

  // Executeable paths.  
  // Use name of exec if in PATH or is symlinked in bin/ of this pipeline
  vep             = "vep"
  counts_exec     = "ComputeAlleleCountsAndHistograms"

  // Scripts use full path or path to symlink in local bin/
  add_cadd_script = "bin/add_cadd_scores.py"
}

process {
  queue = "topmed"
  withLabel: "small_mem" {
    cpus = 1
    time = "2d" 
    memory = "8GB"
  }
  withLabel: "big_mem" {
    cpus = 1
    time = "2d"
    memory = "16GB"         
  }
}

executor {
    
  $slurm {
    queueSize = 1000
  }
  $local {
    cpus = 4 // set number of CPUs to use when running on a single machine
  }
}

// To run on cluster use: nextflow run PrepareVCF.nf -profile slurm
profiles {
  standard {
      process.executor = 'local'
  }

  slurm {
      process.excutor = 'slurm'
  }
}
